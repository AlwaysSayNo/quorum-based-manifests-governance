Admission Control Overhead

Idea: Measuring the additional latency imposed on standard Kubernetes operations by the validation logic. This benchmark measures the Admission Control Overhead: the additional time required for the Kubernetes API Server to process requests when the \ac{QuBManGo} Validating Webhook is active compared to a baseline system without the webhook. E.g. there will be 2 major executions: with and without the qubmango.

We should have:
A test pseudorandom input generator. Input is generated as sequence of kubectl commands and saved as a separate file. The input's actions are generated based on some probabilities and fed into SUT (Kubernetes cluster) with and without Qubmango. The separate script takes the input data file, executes actions one-by-one and measures the total time for all of them. Between each execution some sleep should ocure (let's say 1 second), so that kubernetes could process these changes. The objects to be experimented on: namespaces, deployments and configmaps
The distributions match a Read-Heavy operational model and look like:
- Reads (80%): fetch one deployment/namespace/configmap or non-existing resource
- Updates (10%): Patching existing resources. It includes only changing (adding) annotation on an existing resource
- Creates (7%): Creating new resources.
- Deletes (3%): Removing resources.

The subaction / resource to make operation on is chosen randomly within an action (e.g. what read in the read section (namespace/configmap/deployment/non-existing) has equal chance. The same for other operations create, update, delete)
Except reads. Reads have 30-30-30-10 distribution (10% for non-existing)

All inputs should be valid (e.g. no colliding name-namespace pairs on creation, updated/deleted resources should exist (e.g. generating script should remember, if resources were created and what are their name/namespace))

Protocol:
- Before each run, the cluster is pre-populated with a default dataset (3 namespaces, each containing 1 Deployment and 1 ConfigMap) to ensure that Update and Delete operations target valid existing resources.
- Input Generation: Input files containing sequential actions for batch sizes of N: 1, 10, 50, 100, 500
- Execution: each action is executed separately and summed to the total time. E.g. 1. read command from the rime -> 2. start timer -> 3. execute command -> 4. stop timer -> add the difference to the total time
- Repetition: Each batch size was executed 5 times, and the arithmetic mean was calculated to minimize variance caused by transient network fluctuations.
- Environment: As defined in Section 7.1, the test was run on a local Minikube cluster. The performance baseline relies on the underlying hardware resources allocated to the VM.


Technical details:

- pseudorandom test data generator:
    - Go lang executable (e.g. simple main.go compiled file).
    - can make operations on 3 types of resources: namespace, configmap, deployment (some simple container without any heavy operations)
    - input parameters:
        - changes passed as int values (in our example 80, 11, 7, 2) and should sum-up to 100
        - the number of generated commands (additionally to the init block)
        - the init block and main block should be have their own headers (to separate them in execution script)
        - the actions should be on existing resources (except non-existing read or creation). E.g. generating script should remember the resources name/namespace and generate correct random ones on creation
    
- execution script:
    - Go lang executable
    - have input parameter: a path to the script file
    - should execute the init block, wait 5 second and then execute the main block
    - for each creation sleep 1 second
    - it measures only the execution of commands (not the whole execution of the script) and only from the main block